This project implements both extractive and abstractive text summarization techniques to generate concise and meaningful summaries from long text documents. The extractive model identifies the most relevant sentences from the input, while the abstractive model generates human-like summaries by rephrasing and restructuring the content.

Programming Language: Python

Libraries & Frameworks: NLTK, Hugging Face Transformers, PyTorch

Models Used: BERT (Bidirectional Encoder Representations from Transformers)

Extractive Summarization:

Implemented using NLTK.

Applied tokenization, stop word removal, and word frequency scoring.

Selected the top-ranked sentences to form the final summary.

Abstractive Summarization:

Fine-tuned the BERT-based transformer model.

Generated concise summaries with improved readability and contextual accuracy.
Results:
Extractive Model: Generates summaries by directly selecting sentences from the text.

Abstractive Model: Produces summaries that are rephrased and closer to how humans write.

Achievements

Successfully implemented both extractive and abstractive summarization methods.

Improved summarization quality using fine-tuned BERT model.
