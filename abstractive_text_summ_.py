# -*- coding: utf-8 -*-
"""abstractive text summ .ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1Zneu55sPRZPOyGp70FXnRvk4YA3GkYf_
"""

!pip install datasets

from datasets import load_dataset

ds = load_dataset("knkarthick/dialogsum")

ds

ds['train'][1]

!pip install transformers

"""# WITHOUT FINE TUNING"""

# Use a pipeline as a high-level helper
from transformers import pipeline

pipe = pipeline("summarization", model="facebook/bart-large-cnn")

text1=ds['train'][1]['dialogue']
text1

text1_summary=ds['train'][1]['summary']
text1_summary

pipe(text1, max_length=20,min_length=10,do_sample=False)

"""# WITH FINE TUNING"""

# Load model directly
from transformers import AutoTokenizer, AutoModelForSeq2SeqLM
from transformers import TrainingArguments, Trainer

tokenizer = AutoTokenizer.from_pretrained("facebook/bart-base")

model = AutoModelForSeq2SeqLM.from_pretrained("facebook/bart-base")

#tokenization

def preprocess_function(batch):
    source = batch['dialogue']
    target = batch["summary"]
    source_ids = tokenizer(source, truncation=True, padding="max_length", max_length=128)
    target_ids = tokenizer(target, truncation=True, padding="max_length", max_length=128)

    # Replace pad token id with -100 for labels to ignore padding in loss computation
    labels = target_ids["input_ids"]
    labels = [[(label if label != tokenizer.pad_token_id else -100) for label in labels_example] for labels_example in labels]

    return {
        "input_ids": source_ids["input_ids"],
        "attention_mask": source_ids["attention_mask"],
        "labels": labels
    }

df_source = ds.map(preprocess_function, batched=True)

import os
os.environ["WANDB_DISABLED"] = "true"

# Define training arguments
training_args = TrainingArguments(
    output_dir="/content",  # Replace with your output directory
    per_device_train_batch_size=8,
    num_train_epochs=2,  # Adjust number of epochs as needed
    remove_unused_columns=False
)

# Create Trainer object
trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=df_source["train"],
    eval_dataset=df_source["test"]
)

trainer.train()

# Evaluate the model
eval_results = trainer.evaluate()

# Print evaluation results
print(eval_results)