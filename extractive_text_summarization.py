# -*- coding: utf-8 -*-
"""extractive text summarization.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1uQG2phv0pOHY5de7IZF892tAx--UYONM
"""

import nltk
from nltk.corpus import stopwords
from nltk.tokenize import word_tokenize
from nltk.tokenize import sent_tokenize
import string
import re
# Uncomment these lines if you haven't already downloaded the required NLTK data.
nltk.download('punkt_tab')
nltk.download('stopwords')

text = """In a world often dominated by negativity, it's important to remember the power of kindness and compassion. Small acts of kindness have the ability to brighten someone's day, uplift spirits, and create a ripple effect of positivity that can spread far and wide. Whether it's a smile to a stranger, a helping hand to a friend in need, or a thoughtful gesture to a colleague, every act of kindness has the potential to make a difference in someone's life.Beyond individual actions, there is also immense power in collective efforts to create positive change. When communities come together to support one another, incredible things can happen. From grassroots initiatives to global movements, people are uniting to tackle pressing social and environmental issues, driving meaningful progress and inspiring hope for a better future.It's also important to recognize the strength that lies within each and every one of us. We all have the ability to make a positive impact, no matter how small our actions may seem. By tapping into our innate compassion and empathy, we can cultivate a culture of kindness and empathy that enriches our lives and those around us.So let's embrace the power of kindness, and strive to make the world a better place one small act at a time. Together, we can create a brighter, more compassionate future for all."""
text = re.sub(r"\'s\b", " is", text)
# Tokenize the text into words.
tokens = sent_tokenize(text)
print(tokens)
# Get the list of English stop words and punctuation.
stop_words = set(stopwords.words('english'))
punctuation = set(string.punctuation)

f_token=[]
for i in range(len(tokens)):
    words=word_tokenize(tokens[i])

    filtered_token=[
        token.lower() for token in words
        if token.lower() not in stop_words and token not in punctuation and token != '\n'
    ]

    f_token.extend(filtered_token)




print(f_token)

from collections import Counter

word_freq=Counter(f_token)
word_freq

sorted_counts = sorted(word_freq.items(), key=lambda x: x[1], reverse=True)

# Printing the sorted word frequencies
for word, freq in sorted_counts:
    print(f"{word}: {freq}")

maxx=max(word_freq.values())

maxx

"""**Normalizing""
"""

for i in word_freq.keys():
  word_freq[i]=word_freq[i]/maxx
word_freq

text = re.sub(r'(?<=[a-zA-Z])\.(?=[A-Z])', '. ', text)
tokens=sent_tokenize(text)
tokens

sent_tokens={}
for sent in tokens:
  for word in sent.split():
    if sent in sent_tokens.keys():
      sent_tokens[sent]+=word_freq[word]
    else:
      sent_tokens[sent]=word_freq[word]

sent_tokens

import pandas as pd

pd.DataFrame(list(sent_tokens.items()),columns=['sent','score'])

from heapq import nlargest

num_sentences=3
n=nlargest(num_sentences,sent_tokens,key=sent_tokens.get)
" ".join(n)